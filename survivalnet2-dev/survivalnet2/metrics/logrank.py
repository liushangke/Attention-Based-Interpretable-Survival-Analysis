import tensorflow as tf
from survivalnet2.data.labels import mask, unstack_labels


class Logrank(tf.keras.metrics.Metric):
    """Calculates a logrank metric for right-censored data. This evaluates
    survival differences across categorical groups, and can be used to
    evaluate binary or multiclass classifications. This comparison can be
    made using hard or soft assignments, with soft assignments representing
    probabalistic assigments of samples to groups. Returns a chi-square
    statistic.

    Parameters
    ----------
    name : string
        The name of the class instance.
    k : int
        The number of groups in the prediction. Default value 2.


    Attributes
    ----------
    expected : tensor (float32)
        The cumulative expected number of events in each group.
    observed: tensor (float32)
        The cumulative observed number of events in each group.
    k : int
        The number of groups in the prediction. Default value 2.

    Notes
    -----
    Bland JM, Altman DG. The logrank test. BMJ. 2004 May 1;328(7447):1073.
    doi: 10.1136/bmj.328.7447.1073. PMID: 15117797; PMCID: PMC403858.
    """

    def __init__(self, name="logrank", k=2, **kwargs):
        super().__init__(name=name, **kwargs)
        self.expected = self.add_weight(
            name="expected", initializer="zeros", shape=(k), dtype=tf.float32
        )
        self.observed = self.add_weight(
            name="observed", initializer="zeros", shape=(k), dtype=tf.float32
        )
        self.k = self.add_weight(name="bins", initializer="zeros", dtype=tf.int32)
        self.k.assign(k)

    def reset_state(self):
        self.expected.assign(tf.zeros((self.k), dtype=tf.float32))
        self.observed.assign(tf.zeros((self.k), dtype=tf.float32))

    def update_state(self, y_true, y_pred, sample_weight=None):
        """Updates the internal state incrementing the expected and observed
        events in each group.

        Parameters
        ----------
        y_true : float
            An N x 2 float32 tensor where event or last followup times are in
            the first column and event indicators are in the second column.
        y_pred : float
            An N x k tensor containing either soft assignments for each sample
            to the k groups, or a one-hot encoding of hard group assignments.
            For soft assignments each row should sum to one. These predictions
            are typically generated by a clustering layer.
        """

        # mask and unpack the labels
        masked, keep = mask(y_true)
        times, events = unstack_labels(masked)

        # Mask the prediction scores
        y_pred = tf.boolean_mask(y_pred, keep, axis=0)

        # calculate actual (hard) or expected (soft) observed events in groups
        observed = tf.reduce_sum(tf.boolean_mask(y_pred, events, axis=0), axis=0)

        # calculate expectation of events at unique times, get events at each time
        unique, _, events_at_unique = tf.unique_with_counts(times[events])

        # at_risk indicator matrix size(times) x size(unique)
        at_risk_indicator = tf.reshape(times, shape=[tf.size(times), 1]) >= tf.reshape(
            unique, shape=[1, tf.size(unique)]
        )
        at_risk = tf.reduce_sum(tf.cast(at_risk_indicator, tf.int32), axis=0)

        # calculate event risk at event times
        event_risk = tf.divide(
            tf.cast(events_at_unique, tf.float32), tf.cast(at_risk, tf.float32)
        )

        # calculate expected events
        weights = tf.reduce_sum(
            tf.cast(at_risk_indicator, tf.float32) * event_risk, axis=1
        )
        expected = tf.reduce_sum(tf.expand_dims(weights, 1) * y_pred, axis=0)

        # update state
        self.expected.assign_add(expected)
        self.observed.assign_add(observed)

    def result(self):
        return tf.reduce_sum(tf.square(self.observed - self.expected) / self.expected)
