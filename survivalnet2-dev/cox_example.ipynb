{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cox regression and multitask learning\n",
    "\n",
    "This notebook demonstrates how to get started with SurvivalNet using a simple Cox regression model. \n",
    "\n",
    "A dataset containing protein expression profiles for gliomas from TCGA is provided. Two outcomes are available: 1. Overall survival (OS) and 2. Progression free interval (PFI). First, we show how a simple Keras model can be trained using a Cox Efron loss to optimize the partial likelihood of PFI. Then, we develop a two-task model that learns from both PFI and OS to improve prediction accuracy.\n",
    "\n",
    "Topics covered in this notebook:\n",
    "1. Data formatting\n",
    "2. Applying SurvivalNet losses to train Keras models\n",
    "3. Handling of missing/NaN labels\n",
    "4. Using SurvivalNet metrics to monitor training and evaluate performance\n",
    "5. Multi-task learning\n",
    "6. Generating Kaplan-Meier plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import survivalnet2\n",
    "from survivalnet2.data.labels import stack_labels, unstack_labels\n",
    "from survivalnet2.losses import efron\n",
    "from survivalnet2.metrics.concordance import HarrellsC\n",
    "from survivalnet2.visualization import km_plot\n",
    "\n",
    "np.random.seed(51)\n",
    "tf.random.set_seed(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "A dataset contains features and labels. In this example, features are represented by an 565 x 412 matrix where each row contains the features for one patient. The PFI and OS labels are each represented by a 565 x 2 matrix, where the first column represents the event or last followup time, and the second column contains the event indicator (1 for samples where the event was observed). These label and data formats are used throughout SurvivalNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example(file):\n",
    "    # load example data, generate random train/test split\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    # retrieve protein expression features\n",
    "    features = data.iloc[13:, :].to_numpy().T\n",
    "\n",
    "    # get outcomes\n",
    "    osr = data.iloc[[6, 5], :].to_numpy().T\n",
    "    pfi = data.iloc[[12, 11], :].to_numpy().T\n",
    "\n",
    "    # convert types\n",
    "    features = features.astype(np.float32)\n",
    "    osr = osr.astype(np.float32)\n",
    "    pfi = pfi.astype(np.float32)\n",
    "\n",
    "    return features, osr, pfi\n",
    "\n",
    "\n",
    "# add package install path to python\n",
    "install_dir = os.path.dirname(os.path.dirname(survivalnet2.__file__))\n",
    "sys.path.append(install_dir)\n",
    "\n",
    "# load example data\n",
    "data_path = os.path.join(install_dir, \"examples/TCGA_glioma.csv\")\n",
    "features, osr, pfi = load_example(data_path)\n",
    "# print(features.shape, features)\n",
    "# print(osr.shape, osr)\n",
    "# print(pfi.shape, pfi)\n",
    "# get data shape\n",
    "(N, D) = features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate missing labels\n",
    "\n",
    "Datasets often have missing labels. In a multitask learning problem where we are training with both OS and PFI, we may have some samples that have one label but are missing the other. As long as one label is available, a sample can be used in training.\n",
    "\n",
    "Here, we simulate missing labels by randomly deleting 10% of labels from OS and PFI. SurvivalNet losses implement masking, so that `NaN` time or event values are treated as missing labels. This masking is convenient for users and allows utilization of samples in datasets with sparse labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly discard 10% of OS labels and 10% PFI labels\n",
    "osr[np.random.choice(N, np.round(0.1 * D).astype(np.int32)), :] = np.nan\n",
    "pfi[np.random.choice(N, np.round(0.1 * D).astype(np.int32)), :] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate PFI-only model using Cox Efron loss\n",
    "\n",
    "After splitting the data into training and testing sets, we create a tf.data.Dataset object that can emit batches for training. A two layer model is built using the Keras functional interface and trained with the Cox proportional hazards loss with Efron approximation to handle tied times. This model predicts a dimensionless risk score that can be used to rank samples in terms of predicted outcomes, with higher scores corresponding to worse predicted outcomes.\n",
    "\n",
    "The trained model is evaluated on the held-out test samples using Harrell's concordance index (c-index). c-index measures the concordance between predicted risks and actual outcomes. The predicted risk scores are also used to assign test samples to PFI risk categories which are visualized using a Kaplan-Meier plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412,)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 412) for input KerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64, 412]), tf.float32, 0, tf.int64), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, None).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/mil/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/shangke/Desktop/pathology/survivalnet2-dev/survivalnet2/losses/cox.py\", line 213, in efron  *\n        times, events = unstack_labels(masked)\n    File \"/Users/shangke/Desktop/pathology/survivalnet2-dev/survivalnet2/data/labels.py\", line 62, in unstack_labels  *\n        times, events = tf.unstack(labels, axis=1)\n\n    ValueError: Cannot infer argument `num` from shape (None, None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     23\u001b[0m     loss\u001b[39m=\u001b[39mefron,\n\u001b[1;32m     24\u001b[0m     metrics\u001b[39m=\u001b[39m[HarrellsC()],\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m),\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[39m# model.fit(x=dataset.batch(64), epochs=200, verbose=0)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# evaluate on testing data\u001b[39;00m\n\u001b[1;32m     31\u001b[0m risks \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_loader)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mil/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mil/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/mil/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/shangke/Desktop/pathology/survivalnet2-dev/survivalnet2/losses/cox.py\", line 213, in efron  *\n        times, events = unstack_labels(masked)\n    File \"/Users/shangke/Desktop/pathology/survivalnet2-dev/survivalnet2/data/labels.py\", line 62, in unstack_labels  *\n        times, events = tf.unstack(labels, axis=1)\n\n    ValueError: Cannot infer argument `num` from shape (None, None)\n"
     ]
    }
   ],
   "source": [
    "# generate train/test split\n",
    "index = np.argsort(np.random.rand(N))\n",
    "train = np.zeros(N, np.bool_)\n",
    "train[index[0 : np.int32(0.8 * N)].astype(np.int32)] = True\n",
    "test = ~train\n",
    "\n",
    "# create tf Dataset for Keras training\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((features[train, :], pfi[train, :]))\n",
    "\n",
    "# create tf Dataset for Keras training\n",
    "train_loader = SurvivalDataLoader(features, pfi, batch_size=64, shuffle=True)\n",
    "test_loader = SurvivalDataLoader(features, pfi, batch_size=64, shuffle=False)\n",
    "\n",
    "# build a simple 2 layer model\n",
    "inputs = tf.keras.Input(shape=(64, 412), ragged=True)\n",
    "beta1 = tf.keras.layers.Dense(units=10, activation=\"selu\")\n",
    "beta2 = tf.keras.layers.Dense(units=1, activation=\"linear\")\n",
    "outputs = beta2(beta1(inputs))\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# train PFI network using cox loss\n",
    "model.compile(\n",
    "    loss=efron,\n",
    "    metrics=[HarrellsC()],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "# model.fit(x=dataset.batch(64), epochs=200, verbose=0)\n",
    "model.fit(x=train_loader, epochs=200, verbose=0)\n",
    "# evaluate on testing data\n",
    "risks = model.predict(test_loader)\n",
    "cindex = HarrellsC()\n",
    "print(\"Testing c-index: %0.3f\" % cindex(pfi[test, :], risks))\n",
    "\n",
    "# visualize\n",
    "risk_groups = np.squeeze(np.array(risks > np.median(risks), np.int32)) + 1\n",
    "km_plot(\n",
    "    np.array(pfi[test, :]),\n",
    "    groups=risk_groups,\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Progression probability\",\n",
    "    legend=[\"predicted low risk\", \"predicted high risk\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate PFI+OS multitask model using Cox Efron loss\n",
    "\n",
    "To improve performance, we build a new model that has a single shared layer, followed by independent layers for PFI and OS prediction. We train this model using equally-weighted Efron losses for PFI and OS, and evaluate its accuracy in predicting PFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[209], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# # create tf Dataset for Keras training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# dataset = tf.data.Dataset.from_tensor_slices(\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#     (features[train, :], (pfi[train, :], osr[train, :]))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loader \u001b[39m=\u001b[39m SurvivalDataLoader(features, pfi, osr, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m test_loader \u001b[39m=\u001b[39m SurvivalDataLoader(features, pfi, osr, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# build a simple 2 layer model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "# # create tf Dataset for Keras training\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#     (features[train, :], (pfi[train, :], osr[train, :]))\n",
    "# )\n",
    "\n",
    "train_loader = SurvivalDataLoader(features, pfi, osr, batch_size=64, shuffle=True)\n",
    "test_loader = SurvivalDataLoader(features, pfi, osr, batch_size=64, shuffle=False)\n",
    "\n",
    "# build a simple 2 layer model\n",
    "inputs = tf.keras.Input((features.shape[1],))\n",
    "beta1 = tf.keras.layers.Dense(units=10, activation=\"selu\")\n",
    "beta_pfi = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"pfi\")\n",
    "beta_osr = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"os\")\n",
    "output1 = beta_pfi(beta1(inputs))\n",
    "output2 = beta_osr(beta1(inputs))\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[output1, output2])\n",
    "\n",
    "# train PFI network using cox efron loss and Harrell's c-index as a metric\n",
    "model.compile(\n",
    "    loss=[efron, efron],\n",
    "    metrics=[HarrellsC()],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    ")\n",
    "model.fit(x=train_loader, epochs=200, verbose=0)\n",
    "\n",
    "# evaluate on testing data\n",
    "risks = model(features[test, :])[1]\n",
    "cindex = HarrellsC()\n",
    "print(\"Testing c-index: %0.3f\" % cindex(pfi[test, :], risks))\n",
    "\n",
    "# visualize\n",
    "risk_groups = np.squeeze(np.array(risks > np.median(risks), np.int)) + 1\n",
    "km_plot(\n",
    "    np.array(pfi[test, :]),\n",
    "    groups=risk_groups,\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Progression probability\",\n",
    "    legend=[\"predicted low risk\", \"predicted high risk\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d854e9744e83da29a77f4ba6ba18dc9ff28eb6d36703560718d54df47b94bd68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
