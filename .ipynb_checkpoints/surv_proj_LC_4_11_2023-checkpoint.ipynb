{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsNSqdFeOkBz"
   },
   "source": [
    "### INSTALL PACKAGES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "SVQf74uYOq9d",
    "outputId": "937c4e65-e481-4a3b-f1f4-bb0ab9b012c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./survivalnet2\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting glimr@ git+https://github.com/PathologyDataScience/glimr\n",
      "  Cloning https://github.com/PathologyDataScience/glimr to /private/var/folders/tz/qttd962d27n1g_l3f83f9s95byzb9n/T/pip-install-ieb1dnau/glimr_4785482cfdee4cdaa873699a831d6db0\n",
      "  Running command git clone -q https://github.com/PathologyDataScience/glimr /private/var/folders/tz/qttd962d27n1g_l3f83f9s95byzb9n/T/pip-install-ieb1dnau/glimr_4785482cfdee4cdaa873699a831d6db0\n",
      "  Resolved https://github.com/PathologyDataScience/glimr to commit 0813ce7644ea117286b21133d5a3b0894ba2ca68\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from survivalnet2==0.1.dev591+ga358224) (2.0.0)\n",
      "Requirement already satisfied: numpy in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from survivalnet2==0.1.dev591+ga358224) (1.22.4)\n",
      "Collecting pooch\n",
      "  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from survivalnet2==0.1.dev591+ga358224) (3.7.1)\n",
      "Requirement already satisfied: tensorflow>=2.5 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from survivalnet2==0.1.dev591+ga358224) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.6.3)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.2.1)\n",
      "Requirement already satisfied: setuptools in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (65.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.43.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.32.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.19.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.5.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (13.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.10.0.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (0.4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (2022.12.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5->survivalnet2==0.1.dev591+ga358224) (3.2.0)\n",
      "Collecting ray[air,tune]>=2.3.0\n",
      "  Downloading ray-2.3.1-cp39-cp39-macosx_10_15_x86_64.whl (78.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 78.0 MB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: virtualenv>=20.0.24 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (20.16.3)\n",
      "Requirement already satisfied: filelock in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (3.8.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (1.0.2)\n",
      "Requirement already satisfied: attrs in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (21.2.0)\n",
      "Requirement already satisfied: frozenlist in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (1.3.1)\n",
      "Requirement already satisfied: jsonschema in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (3.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (8.0.3)\n",
      "Requirement already satisfied: aiosignal in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (6.0)\n",
      "Requirement already satisfied: tabulate in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (0.8.10)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (2.5.1)\n",
      "Collecting pyarrow>=6.0.1\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-macosx_10_14_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorful\n",
      "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiorwlock\n",
      "  Downloading aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting opencensus\n",
      "  Downloading opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (0.11.0)\n",
      "Collecting gpustat>=1.0.0\n",
      "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (2021.8.1)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp>=3.7\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-macosx_10_9_x86_64.whl (360 kB)\n",
      "\u001b[K     |████████████████████████████████| 360 kB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-ml-py>=11.450.129\n",
      "  Downloading nvidia_ml_py-11.525.112-py3-none-any.whl (35 kB)\n",
      "Collecting blessed>=1.17.1\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.6.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from gpustat>=1.0.0->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (5.8.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from pandas->survivalnet2==0.1.dev591+ga358224) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from pandas->survivalnet2==0.1.dev591+ga358224) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from pandas->survivalnet2==0.1.dev591+ga358224) (2.8.2)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (2.5.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (0.3.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (1.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->ray[air,tune]>=2.3.0->glimr@ git+https://github.com/PathologyDataScience/glimr->survivalnet2==0.1.dev591+ga358224) (0.18.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (4.39.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/lac5440/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->survivalnet2==0.1.dev591+ga358224) (1.0.7)\n",
      "Collecting opencensus-context>=0.1.3\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 34.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 28.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-macosx_10_9_x86_64.whl (980 kB)\n",
      "\u001b[K     |████████████████████████████████| 980 kB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[K     |████████████████████████████████| 223 kB 37.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: survivalnet2, glimr, gpustat\n",
      "  Building wheel for survivalnet2 (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for survivalnet2: filename=survivalnet2-0.1.dev591+ga358224-py3-none-any.whl size=42217 sha256=5fa8141a17a92058230558ee7f5b3383ea97f427328484246a52675892cf69af\n",
      "  Stored in directory: /private/var/folders/tz/qttd962d27n1g_l3f83f9s95byzb9n/T/pip-ephem-wheel-cache-mncgjxco/wheels/37/8c/c1/09fc4787f07e78a20dd6e30aba178a850b3d18343edbd9e123\n",
      "  Building wheel for glimr (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for glimr: filename=glimr-0.1.dev68+g0813ce7-py3-none-any.whl size=18688 sha256=4340388e0de9a70caee66dda3632ce1682bf6ee67953dae83de7377237ace3f1\n",
      "  Stored in directory: /private/var/folders/tz/qttd962d27n1g_l3f83f9s95byzb9n/T/pip-ephem-wheel-cache-mncgjxco/wheels/2a/13/df/98be7159619826b217d557ba0a630c2be0f703b3fe5f45b647\n",
      "  Building wheel for gpustat (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26280 sha256=fe319671aca971c867a3c1d3aa0d1068d34f06158c1dc528d78644dfb2a7a7a2\n",
      "  Stored in directory: /Users/lac5440/Library/Caches/pip/wheels/91/f0/b3/8566d6821307110981a5db015cbf8fd88697446f81e5f40a27\n",
      "Successfully built survivalnet2 glimr gpustat\n",
      "Installing collected packages: protobuf, multidict, yarl, typing-extensions, googleapis-common-protos, google-auth, async-timeout, anyio, starlette, pydantic, opencensus-context, nvidia-ml-py, h11, google-api-core, blessed, aiohttp, uvicorn, smart-open, ray, pyarrow, py-spy, opencensus, gpustat, fastapi, colorful, aiorwlock, aiohttp-cors, pooch, glimr, survivalnet2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.6.0\n",
      "    Uninstalling google-auth-2.6.0:\n",
      "      Successfully uninstalled google-auth-2.6.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 2.2.0\n",
      "    Uninstalling anyio-2.2.0:\n",
      "      Successfully uninstalled anyio-2.2.0\n",
      "  Attempting uninstall: ray\n",
      "    Found existing installation: ray 2.2.0\n",
      "    Uninstalling ray-2.2.0:\n",
      "      Successfully uninstalled ray-2.2.0\n",
      "Successfully installed aiohttp-3.8.4 aiohttp-cors-0.7.0 aiorwlock-1.3.0 anyio-3.6.2 async-timeout-4.0.2 blessed-1.20.0 colorful-0.5.5 fastapi-0.95.0 glimr-0.1.dev68+g0813ce7 google-api-core-2.11.0 google-auth-2.17.2 googleapis-common-protos-1.59.0 gpustat-1.1 h11-0.14.0 multidict-6.0.4 nvidia-ml-py-11.525.112 opencensus-0.11.2 opencensus-context-0.1.3 pooch-1.7.0 protobuf-3.19.6 py-spy-0.3.14 pyarrow-11.0.0 pydantic-1.10.7 ray-2.3.1 smart-open-6.3.0 starlette-0.26.1 survivalnet2-0.1.dev591+ga358224 typing-extensions-4.5.0 uvicorn-0.21.1 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "# Install survivalnet2 package\n",
    "!pip install ./survivalnet2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import survivalnet2\n",
    "from survivalnet2.data.labels import stack_labels, unstack_labels\n",
    "from survivalnet2.losses import efron\n",
    "from survivalnet2.metrics.concordance import HarrellsC\n",
    "from survivalnet2.visualization import km_plot\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(51)\n",
    "tf.random.set_seed(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcVAoYqxOtRa"
   },
   "source": [
    "### DATA PREPROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9q-JFZRNO39Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 48\n"
     ]
    }
   ],
   "source": [
    "# Define dimensionality\n",
    "D = 48\n",
    "print(f\"Dimensionality: {D}\")\n",
    "\n",
    "# Define the batch size you want to use\n",
    "batch_size = 8\n",
    "\n",
    "# Create data dirs\n",
    "data_dir = '/Users/lac5440/Desktop/CPSII_40X'# Modify this to your data dir\n",
    "csv_names = os.listdir(data_dir)\n",
    "data_files = [os.path.join(data_dir, str(csv_name)) for csv_name in csv_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4zu4JnEPAdg"
   },
   "source": [
    "### RAGGED DATALOADER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "E3GjSRJvPIkM"
   },
   "outputs": [],
   "source": [
    "def dataloader(data_files):\n",
    "    rows_list = []\n",
    "    time_list = []\n",
    "    event_list = []\n",
    "    for data_file in data_files:\n",
    "        df = pd.read_csv(data_file)\n",
    "        num_rows = df.shape[0]\n",
    "        if(num_rows == 0):\n",
    "            continue\n",
    "        df = df.iloc[:, 3:]  # Drop the first three columns\n",
    "        df = df.astype('float32')  # Convert all columns to float32\n",
    "        rows_list.append(df.values)\n",
    "        time_list.append(np.random.randint(50, 300, size=(1,1)).astype('float32'))\n",
    "        event_list.append(np.random.randint(0, 2, size=(1,1)).astype('float32'))\n",
    "    \n",
    "    time_list = np.concatenate(time_list, axis=0)\n",
    "    event_list = np.concatenate(event_list, axis=0)\n",
    "    labels = stack_labels(tf.convert_to_tensor(time_list), tf.convert_to_tensor(event_list))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    rows_tensor = tf.ragged.constant(rows_list, ragged_rank=1, dtype=tf.float32)\n",
    "    \n",
    "    return rows_tensor, labels\n",
    "\n",
    "# Load data from csv files\n",
    "data, labels = dataloader(data_files)\n",
    "\n",
    "# Convert data to a TensorFlow dataset\n",
    "ds = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "# Define a function to transform dense dataset to ragged dataset\n",
    "def dense_to_ragged(dense, D, batch_size):\n",
    "    \n",
    "    # Transform to ragged dataset\n",
    "    ragged = dense.apply(\n",
    "        tf.data.experimental.dense_to_ragged_batch(\n",
    "            batch_size=batch_size, drop_remainder=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return ragged\n",
    "\n",
    "# Transform dataset to a ragged dataset with the defined batch size\n",
    "ragged = dense_to_ragged(ds, D, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MnSXC5WP70r"
   },
   "source": [
    "### MODEL ARCHITECTURE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ymAdL_89P_9J"
   },
   "outputs": [],
   "source": [
    "def build_model(D):\n",
    "    # build a simple 2 layer model\n",
    "    inputs = tf.keras.layers.Input(shape=(None, D), ragged=True)\n",
    "    beta1 = tf.keras.layers.Dense(units=10, activation=\"selu\")\n",
    "    beta_time = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"time\")\n",
    "    beta_event = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"event\") \n",
    "    \n",
    "    output1 = beta_time(beta1(inputs))\n",
    "    output2 = beta_event(beta1(inputs))\n",
    "    output1 = tf.keras.layers.GlobalAveragePooling1D()(output1)\n",
    "    output2 = tf.keras.layers.GlobalAveragePooling1D()(output2)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=(output1, output2))\n",
    "\n",
    "    print(model.input_shape)\n",
    "    print(model.output_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4lfVhCfPZbg"
   },
   "source": [
    "### MODEL TRAINING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JVMVzoNbPhfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 48)\n",
      "((None, 1), (None, 1))\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 5s 14ms/step - loss: 95577.0781 - global_average_pooling1d_2_loss: 22595.5176 - global_average_pooling1d_3_loss: 72981.5469 - global_average_pooling1d_2_harrellsc: 0.4930 - global_average_pooling1d_3_harrellsc: 0.4822\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 85046.7422 - global_average_pooling1d_2_loss: 17006.5449 - global_average_pooling1d_3_loss: 68040.2109 - global_average_pooling1d_2_harrellsc: 0.4978 - global_average_pooling1d_3_harrellsc: 0.4855\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 74392.8672 - global_average_pooling1d_2_loss: 14169.6426 - global_average_pooling1d_3_loss: 60223.2227 - global_average_pooling1d_2_harrellsc: 0.5018 - global_average_pooling1d_3_harrellsc: 0.4922\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 65302.5938 - global_average_pooling1d_2_loss: 12600.8398 - global_average_pooling1d_3_loss: 52701.7617 - global_average_pooling1d_2_harrellsc: 0.5057 - global_average_pooling1d_3_harrellsc: 0.4884\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 56685.4688 - global_average_pooling1d_2_loss: 11789.1855 - global_average_pooling1d_3_loss: 44896.2852 - global_average_pooling1d_2_harrellsc: 0.5125 - global_average_pooling1d_3_harrellsc: 0.4801\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 54181.3555 - global_average_pooling1d_2_loss: 10877.9131 - global_average_pooling1d_3_loss: 43303.4492 - global_average_pooling1d_2_harrellsc: 0.5067 - global_average_pooling1d_3_harrellsc: 0.4836\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 45458.6602 - global_average_pooling1d_2_loss: 9806.6113 - global_average_pooling1d_3_loss: 35652.0469 - global_average_pooling1d_2_harrellsc: 0.5107 - global_average_pooling1d_3_harrellsc: 0.4883\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 40743.0938 - global_average_pooling1d_2_loss: 8714.7480 - global_average_pooling1d_3_loss: 32028.3516 - global_average_pooling1d_2_harrellsc: 0.5076 - global_average_pooling1d_3_harrellsc: 0.4875\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 36349.6406 - global_average_pooling1d_2_loss: 7399.7905 - global_average_pooling1d_3_loss: 28949.8457 - global_average_pooling1d_2_harrellsc: 0.5146 - global_average_pooling1d_3_harrellsc: 0.4938\n",
      "Epoch 10/200\n",
      "21/53 [==========>...................] - ETA: 0s - loss: 32693.1250 - global_average_pooling1d_2_loss: 6930.0498 - global_average_pooling1d_3_loss: 25763.0781 - global_average_pooling1d_2_harrellsc: 0.5219 - global_average_pooling1d_3_harrellsc: 0.4909"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tz/qttd962d27n1g_l3f83f9s95byzb9n/T/ipykernel_32502/1252751747.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the model using the zipped dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model(D)\n",
    "\n",
    "model.compile(\n",
    "    loss=[efron, efron],\n",
    "    metrics=[HarrellsC()],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "# Train the model using the zipped dataset\n",
    "model.fit(data.to_tensor(), labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention model\n",
    "\n",
    "Here is what I could work out below. Once we set the last dimension above in `tf.ragged.constant` to 48, we can apply layers to the data batches.\n",
    "\n",
    "Suppose we have `N_i` region vectors for patient `i`. Then with 1694 patients, the data has shape `[1694, None, 48]`, where `None` is the ragged dimension.\n",
    "\n",
    "This code calculates an attention weight from each 48-element region feature vector to create a `[1694, None, 1]` set of attention weights (one weight per region per subject). We then normalize these within each subject to sum to 1, producing `normalized`. Within each subject, we then multiply the attention weights by the region feature vectors to produce a single 48-dimensional feature vector for each subject (`pooled.shape` is `[1694, 1, 48]`). Finally, we apply a simple linear model to the pooled vector to produce the scalar `risk` that is used as input to the `cox` regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention weights\n",
    "att = tf.keras.layers.Dense(units=1, activation=\"selu\", name=\"att\")(data)\n",
    "# optional - more layers here\n",
    "\n",
    "# normalize weights to sum to 1\n",
    "# we have to expand_dims of totals here for broadcasting to work correctly\n",
    "totals = tf.reduce_sum(att, axis=1, name=\"att_total\")\n",
    "normalized = tf.math.divide_no_nan(att, tf.expand_dims(totals, axis=1), name=\"normalized\")\n",
    "\n",
    "# use attention weights to calculate weighted sum of regions\n",
    "pooled = tf.linalg.matmul(normalized, data, transpose_a=True)\n",
    "\n",
    "# apply a linear layer to the pooled vector to generate the risk value\n",
    "risk = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"risk\")(pooled)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
