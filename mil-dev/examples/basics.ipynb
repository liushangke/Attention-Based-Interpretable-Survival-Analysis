{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers the basic model building, data generation, storage, and loading functions of the library. We start by demonstrating how to extract and save features from whole-slide images using `HistomicsStream` and the `mil.io` subpackage. Then we demonstrate how to build either set-based or structured models from these datasets using the `mil.models` subpackage.\n",
    "\n",
    "Concepts:\n",
    "   - Feature extraction (multi-GPU)\n",
    "   - Wrapping models for histomics stream\n",
    "   - Structured and flattened data formats\n",
    "   - Convolutional and dense WS/MIL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install openslide, histomics_stream, pandas\n",
    "!apt-get update\n",
    "!apt-get install -y openslide-tools\n",
    "!pip install openslide-python\n",
    "!pip install histomics_stream 'large_image[openslide]' scikit_image --find-links https://girder.github.io/large_image_wheels\n",
    "!pip install pandas\n",
    "\n",
    "# install mil library with extra ray[tune]\n",
    "user = '########' #git username\n",
    "token = '################' #personal access token - see https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n",
    "branch = 'dev'\n",
    "!python -m pip install git+https://{user}:{token}@github.com/PathologyDataScience/mil.git@{branch}#egg=mil[ray]\n",
    "        \n",
    "# imports\n",
    "from mil.metrics import Balanced, F1, Mcc, Sensitivity, Specificity\n",
    "from mil.models import convolutional_model, attention_flat, attention_flat_tune\n",
    "from mil.io.reader import read_record, peek\n",
    "from mil.io.transforms import parallel_dataset\n",
    "from mil.io.utils import inference, study\n",
    "from mil.io.writer import write_record\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction parameters\n",
    "\n",
    "Feature extraction parameters have a significant impact on model performance, both in terms of accuracy and the time it takes to train a model. Here, we specify the magnification used to extract features, the size of tiles that features are extracted from, the overlap between these tiles, and the number of tiles contained within each read (chunk). We specify a pre-trained model to use for feature extraction, as well as a set of whole-slide images. A large overlap ensures that important structures will appear whole in at least some tiles, but will significantly increase the amount of data that is saved and subsequently used in training.\n",
    "\n",
    "Parameters are also required for saving the features in .tfr files. We have to define the names of the subject labels stored in the .tfr, and the location to save these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction parameters\n",
    "t=224 # tile size (pixels)\n",
    "overlap=0 # tile overlap (pixels)\n",
    "chunk=1792 # chunk size (pixels)\n",
    "magnification=20 # magnification\n",
    "tile_batch=128 # the number of tiles to batch\n",
    "tile_prefetch=2 # the number of batches to prefetch\n",
    "model_name='EfficientNetV2L' # the pre-trained model for feature extraction\n",
    "svspath = '/data/transplant/nwu/wsi/' # path for the whole-slide images\n",
    "\n",
    "# .tfr saving parameters\n",
    "csvfile = './northwestern/CTOT08_clinical_BiopsyImageKeys_4.27.22.csv' # path for the table containing subject data\n",
    "column_mapping = {'SVS_FileName': 'name', 'G': 'g', 'PTC': 'ptc',\n",
    "                  'V': 'v', 'TG': 'tg', 'CG': 'cg', 'MM': 'mm', 'CI': 'ci',\n",
    "                  'CT': 'ct', 'CV': 'cv', 'I': 'i', 'T': 't', 'AH': 'ah'}\n",
    "csvfile = '/data/transplant/nwu/CTOT08_clinical_BiopsyImageKeys.csv' # path for the table containing subject data\n",
    "\n",
    "outpath = f'/data/renal_allograft/nwu_features/{model_name}_{t}_{overlap}_{magnification}X/' # location to store structured .tfr files\n",
    "if not os.path.exists(outpath):\n",
    "    os.mkdir(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and wrapping the feature extraction model\n",
    "\n",
    "To use HistomicsStream for feature extraction, we have to wrap the feature extraction model so that the tile location information and other metadata can be passed through the model and captured at the output registered to the features. HistomicsStream is used to generate a tf.data.Dataset of tiles, and features are extracted from this using tf.keras.Model.predict. Since predict takes a single input, we combine (tiles, tile_metadata) for passing to the wrapped model. Inside the wrapper these are separated and inference is done on the tiles. Wrapping is necessary to avoid having the tile_metadata discarded. We also add a dummy 'y' variable 0. to be discarded by predict.\n",
    "\n",
    "To enable multi-GPU feature extraction, the feature extraction model is loaded outside the parallel context, and is wrapped inside the parallel context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the wrapped model class\n",
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, extractor, *args, **kwargs):\n",
    "        super(WrappedModel, self).__init__(*args, **kwargs)\n",
    "        self.model = extractor\n",
    "        \n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        return self.model(inputs[0]), inputs[1]\n",
    "    \n",
    "\n",
    "# create the feature extractor model to be wrapped\n",
    "model = tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n",
    "        include_top=False, weights='imagenet', input_shape=(t, t, 3),\n",
    "        pooling='avg')\n",
    "\n",
    "# get dimensionality of extracted features\n",
    "D = model.output_shape[-1]\n",
    "\n",
    "# create a distributed wrapped model\n",
    "with tf.distribute.MirroredStrategy().scope():\n",
    "    \n",
    "    # wrap the model\n",
    "    wrapped_model = WrappedModel(model, name='wrapped_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating .tfr files\n",
    "\n",
    "This cell reads in a subject .csv file to build a table of whole-slide image files and subject labels. Each row of the table defines one subject. In this example each subject has a single whole-slide image, although the library supports multiple images per subject. In that case, the features from multiple images are stored in a single .tfr, along with an array indexing each feature to each slide, and lists of slide names and properties.\n",
    "\n",
    "We iterate over each row of the table, generating a HistomicsStream study, doing inference on the resulting tf.data.Dataset, and saving the features to a .tfr along with tile, slide, and subject metadata.\n",
    "\n",
    "In this example, we write the features in a structured format, which places the feature vector obtained from each tile into a 3D tensor which preserves their spatial organization as found in the slide. This enables us to build convolutional models that can leverage spatial information spanning multiple tiles. Structured format is not supported when there are multiple slides / patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels from csv\n",
    "table = pd.read_csv(csvfile)\n",
    "table = table[list(column_mapping.keys())]\n",
    "table = table.rename(columns=column_mapping)\n",
    "\n",
    "# match table entries to existing files\n",
    "files = [slide for slide in os.listdir(svspath) if os.path.splitext(slide)[1] == '.svs']\n",
    "table = table[table.name.isin(files)].reset_index()\n",
    "\n",
    "# write a tf record for each slide\n",
    "for i, entry in table.iterrows():\n",
    "    \n",
    "    # slide\n",
    "    slide = entry['name']\n",
    "    \n",
    "    # add the subject labels present in the table\n",
    "    label = {l:float(entry[l]) for l in entry.keys() if l != 'name'}\n",
    "    \n",
    "    # we can also add custom metadata as scalars, lists of scalars, and np.ndarrays\n",
    "    label['model_name'] = model_name \n",
    "    label['stain'] = 'periodic acidâ€“schiff'\n",
    "    label['encounters'] = ['2/16/19', '4/1/20']\n",
    "    label['test_results'] = np.array([0.3, 1.1])\n",
    "    label['age'] = 64\n",
    "    \n",
    "    # generate tf record filename\n",
    "    filename = slide + '.' + model_name + '_' + str(t) + '_' + str(magnification) + 'X_2d.tfr'     \n",
    "    \n",
    "    # skip if file exists\n",
    "    if os.path.exists(outpath + filename):\n",
    "        continue\n",
    "    else:\n",
    "        print(slide)\n",
    "    \n",
    "    # create histomics stream study\n",
    "    try:\n",
    "        hs_study = study([svspath+slide], (t, t), (overlap, overlap), (chunk, chunk), magnification)\n",
    "    except:\n",
    "        print('Skipping slide ' + slide + ', slide reading error.')\n",
    "        continue\n",
    "        \n",
    "    # do inference\n",
    "    features, tile_info = inference(wrapped_model, hs_study, batch=tile_batch, prefetch=tile_prefetch)\n",
    "\n",
    "    # write record to disk\n",
    "    write_record(outpath + filename, features, tile_info, label, structured=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a .tfr file\n",
    "\n",
    "`mil.io.reader.peek` inspects the contents of a .tfr file and returns a dictionary of the variable names and types. This can be helpful to inspect datasets and determine the user metadata embedded in the .tfr files.\n",
    "\n",
    "Due to the way tensorflow handles loading .tfr files, this information cannot be acquired at runtime, and so we capture it here in eager mode and provide it to `mil.io.reader.read_record` when training the networks in graph mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get list of created tf.records\n",
    "files = [outpath + file for file in os.listdir(outpath) if os.path.splitext(file)[1] == '.tfr']\n",
    "\n",
    "# inspect contents of one file\n",
    "serialized = list(tf.data.TFRecordDataset(files[0]))[0]\n",
    "variables = peek(serialized)\n",
    "\n",
    "# display variables\n",
    "print(json.dumps(variables, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a convolutional model from structured tensors\n",
    "\n",
    "Here we use the `mil.models` subpackage to build and train a simple convolutional model for the structured tensor. This model uses weighted-average pooling (attention) to pool the convolutional feature maps over the entire image to make a prediction. This enables support for variable-sized images.\n",
    "\n",
    "We use the `mil.io.reader.read_record` function with a `tf.data.Dataset` to read features in structured format. Interpreting the .tfr requires passing in the label names were stored within the file. When we load the data, we pick a single label and threshold that to form a binary classificaiton problem (the original labels range from 0 to 4).\n",
    "\n",
    "We also use several new metrics from the `tf.metrics` subpackage to monitor performance during training. These metrics were implemented to address the specific issues of validating pathology models, and are not available in the TensorFlow core package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of metrics to monitor performance during training\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(),\n",
    "           tf.keras.metrics.AUC(curve='ROC'),\n",
    "           Balanced(threshold=0.5),\n",
    "           F1(threshold=0.5),\n",
    "           Mcc(threshold=0.5),\n",
    "           Sensitivity(threshold=0.5),\n",
    "           Specificity(threshold=0.5)]\n",
    "\n",
    "# create and compile model\n",
    "model = convolutional_model(D)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss={'softmax': tf.keras.losses.BinaryCrossentropy()},\n",
    "              metrics={'softmax': metrics})\n",
    "\n",
    "#define label function for training dataset\n",
    "def threshold(value, key='t', cond=lambda x: x>=2.0):\n",
    "    return tf.one_hot(tf.cast(cond(value[key]), tf.int32), depth=2)\n",
    "\n",
    "# build dataset and train\n",
    "train_ds = tf.data.TFRecordDataset(files, num_parallel_reads=4).shuffle(len(files))\n",
    "train_ds = train_ds.map(lambda x: read_record(x, variables, structured=True))\n",
    "train_ds = train_ds.map(lambda x, y, z, _: (x, threshold(y, 't')[0]))\n",
    "train_ds = train_ds.batch(1).prefetch(2)\n",
    "\n",
    "# train model\n",
    "model.fit(train_ds, batch_size=1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a set-based model from flattened tensors\n",
    "\n",
    "Although the tensors are stored in a structured format, they can also be read in a flattened format using *io.read_record* with structured=False.\n",
    "\n",
    "The *models* subpackage also contains set-based models with dense layers that process each tile independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and compile model\n",
    "model = attention_flat(D)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss={'softmax': tf.keras.losses.BinaryCrossentropy(), \"attention_weights\": None},\n",
    "              metrics={'softmax': metrics, \"attention_weights\": None})\n",
    "\n",
    "# build dataset and train\n",
    "train_ds = tf.data.TFRecordDataset(files, num_parallel_reads=4).shuffle(len(files))\n",
    "train_ds = train_ds.map(lambda x: read_record(x, variables, structured=False))\n",
    "train_ds = train_ds.map(lambda x, y, z, _: (x, threshold(y, 't')[0]))\n",
    "train_ds = train_ds.batch(1).prefetch(2)\n",
    "\n",
    "# train model\n",
    "model.fit(train_ds, batch_size=1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU training\n",
    "\n",
    "Distributed training can be performed by transforming the dataset to address variable image sizes.\n",
    "\n",
    "When creating a model, setting `ragged=True` indicates to the model to expect a ragged dataset where feature tensors with possibly variable dimensions are batched.\n",
    "\n",
    "The function `mil.io.transforms.parallel_dataset` performs the necessary transformation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MirroredStrategy for multi-GPU training\n",
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "\n",
    "# create and compile the model and metrics in the strategy scopes\n",
    "with strategy.scope():\n",
    "    \n",
    "    # create a model with ragged inputs\n",
    "    model = attention_flat(D, ragged=True) \n",
    "    \n",
    "    # metrics will be aggregated across gpus\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(),\n",
    "            tf.keras.metrics.AUC(curve='ROC'),\n",
    "            Balanced(threshold=0.5),\n",
    "            F1(threshold=0.5),\n",
    "            Mcc(threshold=0.5),\n",
    "            Sensitivity(threshold=0.5),\n",
    "            Specificity(threshold=0.5)]\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                loss={'softmax': tf.keras.losses.BinaryCrossentropy()},\n",
    "                metrics={'softmax': metrics})\n",
    "\n",
    "\n",
    "#define label function for training dataset\n",
    "def threshold(value, key='t', cond=lambda x: x>=2.0):\n",
    "    return tf.one_hot(tf.cast(cond(value[key]), tf.int32), depth=2)\n",
    "\n",
    "# build dataset and train\n",
    "train_ds = tf.data.TFRecordDataset(files, num_parallel_reads=strategy.num_replicas_in_sync).shuffle(len(files))\n",
    "train_ds = train_ds.map(lambda x: read_record(x, variables, structured=False))\n",
    "train_ds = train_ds.map(lambda x, y, z, _: (x, threshold(y, 't')[0]))\n",
    "train_ds = parallel_dataset(train_ds, \n",
    "                            D, \n",
    "                            strategy.num_replicas_in_sync,\n",
    "                            structured=False)\n",
    "train_ds = train_ds.prefetch(2)\n",
    "\n",
    "# train model\n",
    "model.fit(train_ds, batch_size=strategy.num_replicas_in_sync, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Hyperparameter tuning for attention_flat model can be performed by creating an object of class `attention_flat_tune` and setting the number of trials `trial_num` and number of allocated GPUs/CPUs per `trial resources_per_trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of attention_flat_tune\n",
    "tuner = attention_flat_tune(trial_num=20, resources_per_trial=1)\n",
    "config = tuner.get_config()\n",
    "config\n",
    "\n",
    "# Modify and set tuning config\n",
    "config[\"dataset_params\"] = {\"files\": files, \"variables\": variables, \"structured\": False}\n",
    "config[\"training_params\"][\"D\"] = D\n",
    "tuner.set_config(config)\n",
    "\n",
    "# Run tuner to look for the best hyperparameters\n",
    "attention_flat_best_params = tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build attention_flat using the best hyperparameters\n",
    "model = attention_flat(D, config=attention_flat_best_params, ragged=False) \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              loss={'softmax': tf.keras.losses.BinaryCrossentropy()},\n",
    "              metrics={'softmax': metrics})\n",
    "\n",
    "# build dataset and train\n",
    "train_ds = tf.data.TFRecordDataset(files, num_parallel_reads=4).shuffle(len(files))\n",
    "train_ds = train_ds.map(lambda x: read_record(x, variables, structured=False))\n",
    "train_ds = train_ds.map(lambda x, y, z, _: (x, threshold(y, 't')[0]))\n",
    "train_ds = train_ds.batch(1).prefetch(2)\n",
    "\n",
    "# train model\n",
    "model.fit(train_ds, batch_size=1, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a440aeac0c89e2af7569e0aa53b64434c4b69eb6285e2b0d174d9bca190d54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
